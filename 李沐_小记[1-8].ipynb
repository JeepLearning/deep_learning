{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "110e8244",
   "metadata": {},
   "source": [
    "# 04 数据处理\n",
    "\n",
    "csv 简称\"Comma-Separated Values\"-------逗号分隔值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165c8ab3",
   "metadata": {},
   "source": [
    "# 05 线性代数\n",
    "\n",
    "## 轴的方向\n",
    "\n",
    "指定 axis 的值即指定方向, 在求和, 求平均中常常会使用到. 其中所对应的是 shape中的索引.\n",
    "\n",
    "首先, 我们先以向量举例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c863cbac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "A = torch.Tensor([1, 2])\n",
    "print(A)\n",
    "A.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010a053b",
   "metadata": {},
   "source": [
    "此时 A 只有一个方向, axis = 0 对应的就是 shape 中的 2.\n",
    "\n",
    "下面我们以矩阵为例."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bb1c49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.],\n",
      "        [5., 6.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "A = torch.Tensor([[1, 2],[3, 4],[5, 6]])\n",
    "print(A)\n",
    "A.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081120ac",
   "metadata": {},
   "source": [
    "此时, A 为矩阵, 则其有两个方向, axis = 0 对应的就是 shape 中的 3, 即矩阵的行;axis= 1, 对应的是 shape 中的 2, 即矩阵的列.\n",
    "\n",
    "可以看出, 其中 axis 其实对应的就是 shape 的输出结果 list 类型的索引."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8608d65f",
   "metadata": {},
   "source": [
    "## 降维\n",
    "\n",
    "1. 求和\n",
    "    降低维度最常用的方法是将计算其元素的和; **其中遵循的原则是:按照哪个维度进行求和, 最终得到的结果就是将哪个维度去掉, 其余维度保持不变.** 因此, 求和会导致维度降低.\n",
    "\n",
    "2. 求均值,\n",
    "    与求和类似, 按照该方向进行求均值.\n",
    "\n",
    "3. 按累加求和, 就是每次计算结果保留下来, 作为元素存储起来\n",
    "\n",
    "在降低维度的过程中, 我们可以通过指定keepdimes参数, 来保证其维度不会降低, 而是将该方向上的维度变为 1.\n",
    "\n",
    "## 点积\n",
    "\n",
    "标量由于不存在该问题, 其对应的都是数乘\n",
    "\n",
    "1. 向量和向量做积\n",
    "    结果为一个数, 采用的是 torch.dot() 语句.\n",
    "\n",
    "2. 矩阵和向量做积\n",
    "    对于矩阵和向量的乘积, 结果是一个矩阵, 采用 torch.mv()\n",
    "    其中 m 代表 matrix, v 代表 vector.\n",
    "\n",
    "3. 矩阵与矩阵做积\n",
    "    矩阵乘矩阵结果显然是矩阵, 采用torch,mm(), 其中 m 代表 matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79d7393",
   "metadata": {},
   "source": [
    "# 08 线性回归+基础优化算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b37d94c",
   "metadata": {},
   "source": [
    "## P1_线性回归\n",
    "\n",
    "### 训练数据\n",
    "该小节中, 我们对求梯度进行了运用, 着重介绍对 $w$ 如何计算偏导数.\n",
    "\n",
    "对单个样本而言, 有考虑下式进行预测房价:\n",
    "\n",
    "$$\\hat{y} = w_1  x_1 + ... + w_d  x_d + b.$$\n",
    " \n",
    "将所有特征放到向量 $\\mathbf{x} \\in \\mathbb{R}^d$ 中，\n",
    "并将所有权重放到向量 $\\mathbf{w} \\in \\mathbb{R}^d$ 中，\n",
    "我们可以用点积形式来简洁地表达模型：\n",
    "\n",
    "$$\\hat{y} = <\\mathbf{w},\\mathbf{x}> + b = \\mathbf{w}^\\top \\mathbf{x} + b.$$\n",
    "\n",
    "向量 $\\mathbf{x}$ 对应于单个数据样本的特征.\n",
    "\n",
    "进一步, 对于多个样本, 用符号表示的矩阵 $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$, 定义为\n",
    " $\\mathbf{X}=(\\mathbf{x_1},\\mathbf{x_2},\\dots,\\mathbf{x_n})^T$.\n",
    "可以很方便地引用我们整个数据集的 $n$ 个样本.其中, $\\mathbf{X}$ 的每一行是一个样本，每一列是一种特征.\n",
    "\n",
    "因此, 对于特征集合 $\\mathbf{X}$, 预测值 $\\hat{\\mathbf{y}} \\in \\mathbb{R}^n$, 定义 $\\hat{\\mathbf{y}}=(\\hat{y}_1,\\hat{y}_2,\\dots,\\hat{y}_n)^T$ 进而可以通过矩阵-向量乘法表示为:\n",
    "\n",
    "$${\\hat{\\mathbf{y}}} = \\mathbf{X} \\mathbf{w} + b.$$\n",
    "\n",
    "### 参数学习\n",
    "\n",
    "- 训练损失\n",
    "\n",
    "$$l (\\mathbf{X}, \\mathbf{y}, \\mathbf{w}, b)=\\frac{1}{2n}\\sum_{i=1}^n {(y_i-<\\mathbf{x_i}, \\mathbf{w}>-b)^2} = \\frac{1}{2n} \\| \\mathbf{y}-\\mathbf{Xw}-b\\|^2.$$\n",
    "\n",
    "其中 $\\mathbf{X}=(\\mathbf{x_1},\\mathbf{x_2},\\dots,\\mathbf{x_n})^T$, $\\mathbf{y}=(y_1,y_2,\\dots,y_n)^T$.\n",
    "\n",
    "- 最小化损失来学习参数\n",
    "\n",
    "$$\\mathbf{w}^*,\\mathbf{b}^* = \\operatorname*{argmin}_{\\mathbf{w}, b}\\  l(\\mathbf{X},\\mathbf{y},\\mathbf{w}, b)$$\n",
    "\n",
    "### 显示解\n",
    "在确定损失函数, 以及待估参数后, 我们首先令 $\\mathbf{X} = [\\mathbf{X}, \\mathbf{1}]\\quad \\mathbf{w} = [\\mathbf{w}, b]^T$ 损失函数可以记作:\n",
    "\n",
    "$$l (\\mathbf{X}, \\mathbf{y}, \\mathbf{w}) = \\frac{1}{2n} \\| \\mathbf{y}-\\mathbf{Xw}\\|^2$$\n",
    "\n",
    "因为我想通过 $\\mathbf{w}$ 的选择来使得损失函数达到最小, 因此, 对损失函数关于 $\\mathbf{w}$ 求导有:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "   \\frac{\\partial}{\\partial \\mathbf{w}} l (\\mathbf{X}, \\mathbf{y}, \\mathbf{w})\n",
    "   &=\\nabla_\\mathbf{w} \\left[\\frac{1}{2n} \\| \\mathbf{y}-\\mathbf{Xw}\\|^2\\right]\\\\\n",
    "   &=\\frac{1}{2n}\\nabla_\\mathbf{w}\\| \\mathbf{y}-\\mathbf{Xw}\\|^2\\\\\n",
    "   &=\\frac{1}{2n}\\nabla_\\mathbf{w}\\left[(\\mathbf{y}-\\mathbf{Xw})^T(\\mathbf{y}-\\mathbf{Xw})\\right]\\\\\n",
    "   &=\\frac{1}{2n}\\nabla_\\mathbf{w}tr\\left[(\\mathbf{y}-\\mathbf{Xw})^T(\\mathbf{y}-\\mathbf{Xw})\\right]\\\\\n",
    "   &=\\frac{1}{2n}\\nabla_\\mathbf{w}tr\\left[(\\mathbf{y}^T-\\mathbf{w}^T \\mathbf{X}^T)(\\mathbf{y}-\\mathbf{Xw})\\right]\\\\\n",
    "   &=\\frac{1}{2n}\\nabla_\\mathbf{w}tr\\left[\\mathbf{y}^T\\mathbf{y}-\\mathbf{y}^T\\mathbf{Xw}-\\mathbf{w}^T \\mathbf{X}^T\\mathbf{y}+\\mathbf{w}^T \\mathbf{X}^T\\mathbf{Xw}\\right]\\\\\n",
    "   &=\\frac{1}{2n}\\nabla_\\mathbf{w}\\left[-2tr(\\mathbf{y}^T\\mathbf{Xw})+tr(\\mathbf{w}^T \\mathbf{X}^T\\mathbf{Xw})\\right]\\\\\n",
    "   &=\\frac{1}{2n}\\left[-2\\mathbf{X}^T\\mathbf{y}+[\\nabla_{\\mathbf{w}^T}(tr(\\mathbf{w}^T \\mathbf{X}^T\\mathbf{Xw})]^T\\right]\\\\\n",
    "   &=\\frac{1}{2n}( 2\\mathbf{X}^T\\mathbf{X}\\mathbf{w}-2\\mathbf{X}^T\\mathbf{y})\\\\\n",
    "   &=\\frac{1}{n}\\mathbf{X}^T(\\mathbf{X}\\mathbf{w}-\\mathbf{y}).\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "因此, 令其等式为 0. 我们可以得到:\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "   &\\frac{\\partial}{\\partial \\mathbf{w}} l (\\mathbf{X}, \\mathbf{y}, \\mathbf{w})=0\\\\\n",
    "   \\Leftrightarrow & \\frac{1}{n}\\mathbf{X}^T(\\mathbf{y}- \\mathbf{X}\\mathbf{w})=0\\\\\n",
    "   \\Leftrightarrow & \\mathbf{w}^* =(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "这样以来, 我们通过求导的方式, 得到 $\\mathbf{w}^*$ 的显示解."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6c1ca5",
   "metadata": {},
   "source": [
    "## P2_基础优化算法\n",
    "\n",
    "- 梯度下降通过不断沿着反向梯度的方向更新求解参数\n",
    "- 小批量随机梯度下降是深度学校默认的求解算法\n",
    "- 两个重要的超参数是批量的大小和学习率 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30568a0",
   "metadata": {},
   "source": [
    "## P3_线性回归从零实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8faa982",
   "metadata": {},
   "source": [
    "@ 小记_1\n",
    "- 对于一个矩阵直接使用len函数, 函数返还的是第一个维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df48502a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "20\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "X = torch.normal(0, 1, (1, 20, 5))\n",
    "# 下述两条语句等价\n",
    "print(len(X))\n",
    "print(X.shape[0])\n",
    "\n",
    "Y = torch.normal(0, 1, (20, 5))\n",
    "# 下述两条语句等价\n",
    "print(len(Y))\n",
    "print(Y.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d899c0de",
   "metadata": {},
   "source": [
    "@ 小记_2\n",
    "\n",
    "yield 与 return 相对，他们都在函数中使用，执行后都返回某种结果.\n",
    "- return 在调用过后, 返回函数运行的结果, 该def程序不在运行, 并且将其中涉及到的局部变量去全部消除\n",
    "- yield 在调用后, 会返回一个可迭代的 generator（生成器）对象, 可以使用 for 循环, 或者next()函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c83f30f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "return_back: 1\n",
      "yield_back: <generator object example_yield at 0x000001D1F2DADCF0>\n"
     ]
    }
   ],
   "source": [
    "# retrun\n",
    "def example_return():\n",
    "    x = 1\n",
    "    return x\n",
    "a = example_return()\n",
    "print(\"return_back:\",a)\n",
    "\n",
    "# yield\n",
    "def example_yield():\n",
    "    x = 1\n",
    "    y = 10\n",
    "    while x < y:\n",
    "        yield x\n",
    "        x += 1\n",
    "a = example_yield()\n",
    "print(\"yield_back:\", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c021ca09",
   "metadata": {},
   "source": [
    "看到了两者的区别, 下面来介绍 for 循环, 以及 next() 函数, 来加深 yield 的印象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ac4e3f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object simple_generator at 0x000001D1F3062900>\n",
      "<class 'generator'>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "调用 next 函数:\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "def simple_generator():\n",
    "    x = 1\n",
    "    yield x\n",
    "    yield x + 1\n",
    "    yield x + 2\n",
    "    \n",
    "a = simple_generator()\n",
    "print(a)\n",
    "print(type(a))\n",
    "\n",
    "# print('-'*100)\n",
    "# print('for 循环:')\n",
    "\n",
    "# for i in a:\n",
    "#     print(i)\n",
    "\n",
    "print('-'*100)\n",
    "print('调用 next 函数:')\n",
    "\n",
    "print(next(a))\n",
    "print(next(a))\n",
    "print(next(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33326d0e",
   "metadata": {},
   "source": [
    "可以看到, simple_generator函数返回一个生成器, 调用next()方法后, 函数开始运行.\n",
    "1. 遇到第一个yield关键字, 返回生成的值 (1), 程序暂停;\n",
    "2. 第二次调用next()方法, 代码从上次暂停的位置开始执行, 并遇到了第二个yield关键字, 再返回生成的值 (2), 程序暂停;\n",
    "3. 第三次调用也是如此, 返回生成的值 (3), 生成器耗尽, 程序终止;\n",
    "\n",
    "二者区别就 yield 和 return 的关系区别了, 可以使用 yield 的函数是一个生成器, 这个生成器有可以使用 next 调用, next 就相当于“下一步”生成哪个数, 这一次的 next 开始的地方是接着上一次的next停止的地方执行的.\n",
    "\n",
    "所以调用 next 的时候, 生成器并不会从函数的开始执行, 只是接着上一步停止的地方开始, 然后遇到yield后, return出要生成的数，此步就结束."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8facc95",
   "metadata": {},
   "source": [
    "@ 小记_3\n",
    "- torch.matmul 函数的说明 [不是torch.mm 的简写]\n",
    "\n",
    "torch.matmul(a, b) 也是一种类似于矩阵相乘操作的 tensor 联乘操作, 一般是高维矩阵 a 和 b 相乘, 但是它可以利用 python 中的广播机制, 处理一些维度不同的 tensor 结构进行相乘操作.\n",
    "\n",
    "该函数在二维时, 与 torch.mm 相同, 但是在遇到高维问题时, torch.mm 并不能处理, 但是 torch.matmul 仍可以通过其广播机制, 对高维 tensor 进行处理."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
